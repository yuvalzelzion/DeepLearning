{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "targil4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8dfba187b7f94dc3befb704501b920e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4fc628ffc7044f47a7cf2b38d5908891",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_651a50c182a44f16996b5e0f93d4a00f",
              "IPY_MODEL_8b289f4c00804214adb03271522a5d5a"
            ]
          }
        },
        "4fc628ffc7044f47a7cf2b38d5908891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "651a50c182a44f16996b5e0f93d4a00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc7d2f98a6154d928a783a6254b2f92d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 111898327,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 111898327,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b4ca344dce6f46febbf14c7504022d7e"
          }
        },
        "8b289f4c00804214adb03271522a5d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95f3c4cc8ac14afeaaa16050d64a3326",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 107M/107M [00:02&lt;00:00, 53.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b08a4710e6b4a1d92d86310b91d5652"
          }
        },
        "bc7d2f98a6154d928a783a6254b2f92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b4ca344dce6f46febbf14c7504022d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95f3c4cc8ac14afeaaa16050d64a3326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b08a4710e6b4a1d92d86310b91d5652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruqq-7eg-kIq"
      },
      "source": [
        "# Example 1 - Face detection and recognition inference pipeline\n",
        "The following example illustrates how to use the facenet_pytorch python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odtrOSwV-q9v"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEu7psFV_GKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2932ee-9811-4531-94c5-90e8420e5082"
      },
      "source": [
        "! pip install facenet-pytorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet-pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovxKEFdL9RXy"
      },
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "workers = 0 if os.name == 'nt' else 4"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO-kGmP0-ukI"
      },
      "source": [
        "### Determine if an nvidia GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPgZQjuk-vzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a71489-f344-4ba1-864b-d1f1fd7812cd"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUiiIkeNBqgk"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCCODdi_nO2"
      },
      "source": [
        "### Define MTCNN module\n",
        "Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
        "\n",
        "See help(MTCNN) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXV4IDAc_q5l"
      },
      "source": [
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56_lZkJI_uSY"
      },
      "source": [
        "### Define Inception Resnet V1 module\n",
        "Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to eval mode.\n",
        "\n",
        "See help(InceptionResnetV1) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8dfba187b7f94dc3befb704501b920e7",
            "4fc628ffc7044f47a7cf2b38d5908891",
            "651a50c182a44f16996b5e0f93d4a00f",
            "8b289f4c00804214adb03271522a5d5a",
            "bc7d2f98a6154d928a783a6254b2f92d",
            "b4ca344dce6f46febbf14c7504022d7e",
            "95f3c4cc8ac14afeaaa16050d64a3326",
            "1b08a4710e6b4a1d92d86310b91d5652"
          ]
        },
        "id": "uwFf-6FN_ym4",
        "outputId": "ad4040a3-8f60-4dad-e30c-0fbcaa299fea"
      },
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dfba187b7f94dc3befb704501b920e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=111898327.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCKXEPn__5KI"
      },
      "source": [
        "### Define a dataset and data loader\n",
        "We add the idx_to_class attribute to the dataset to enable easy recoding of label indices to identity names later one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PscshGDmAayW",
        "outputId": "d299492e-511a-47a4-dc30-dbed7ce8f2b5"
      },
      "source": [
        "! git clone https://github.com/timesler/facenet-pytorch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'facenet-pytorch' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U19u5w5JnKM",
        "outputId": "77a6714c-e74f-4ce2-b94f-f3b944f15b47"
      },
      "source": [
        "%cd facenet-pytorch"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/facenet-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irGQg7L3_9zG",
        "outputId": "40258359-d7d8-4684-91c0-dcef5718a16c"
      },
      "source": [
        "def collate_fn(x):\n",
        "    return x[0]\n",
        "\n",
        "dataset = datasets.ImageFolder('data/test_images')\n",
        "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33BrR8QUC0mz"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m0HGGGsA41H"
      },
      "source": [
        "### Perfom MTCNN facial detection\n",
        "Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The MTCNN forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have MTCNN return all detected faces, set keep_all=True when creating the MTCNN object above.\n",
        "\n",
        "To obtain bounding boxes rather than cropped face images, you can instead call the lower-level mtcnn.detect() function. See help(mtcnn.detect) for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MvSx_eBA5xv",
        "outputId": "07fb474c-52ad-4678-d41f-a69ee7a760e5"
      },
      "source": [
        "aligned = []\n",
        "names = []\n",
        "for x, y in loader:\n",
        "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
        "    if x_aligned is not None:\n",
        "        print('Face detected with probability: {:8f}'.format(prob))\n",
        "        aligned.append(x_aligned)\n",
        "        names.append(dataset.idx_to_class[y])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Face detected with probability: 0.999983\n",
            "Face detected with probability: 0.999934\n",
            "Face detected with probability: 0.999733\n",
            "Face detected with probability: 0.999880\n",
            "Face detected with probability: 0.999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl6SMqG9BA6g"
      },
      "source": [
        "### Calculate image embeddings\n",
        "MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it.\n",
        "\n",
        "For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y46ZwAQQBByP"
      },
      "source": [
        "aligned = torch.stack(aligned).to(device)\n",
        "embeddings = resnet(aligned).detach().cpu()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abub2_riBHgm"
      },
      "source": [
        "### Print distance matrix for classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tfK0z3nBIh_",
        "outputId": "220645e0-0b22-4e28-8eb0-66c1ef94f20d"
      },
      "source": [
        "dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\n",
        "print(pd.DataFrame(dists, columns=names, index=names))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                angelina_jolie  bradley_cooper  ...  paul_rudd  shea_whigham\n",
            "angelina_jolie        0.000000        1.447480  ...   1.429847      1.399074\n",
            "bradley_cooper        1.447480        0.000000  ...   1.013447      1.038684\n",
            "kate_siegel           0.887728        1.313749  ...   1.388377      1.379655\n",
            "paul_rudd             1.429847        1.013447  ...   0.000000      1.100503\n",
            "shea_whigham          1.399074        1.038684  ...   1.100503      0.000000\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zA2CBJsEN0B"
      },
      "source": [
        "# Example 2 - Face tracking pipeline\n",
        "The following example illustrates how to use the facenet_pytorch python package to perform face detection and tracking on an image dataset using MTCNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woHC8XXIEW7_"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2bEYa5HFTsI",
        "outputId": "08eeb582-cda0-4573-f92c-aa64f804b8ad"
      },
      "source": [
        "! pip install mmcv==0.4.3"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mmcv==0.4.3 in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv==0.4.3) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv==0.4.3) (1.19.5)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from mmcv==0.4.3) (2.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv==0.4.3) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3GxaJYWEYfA"
      },
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "import numpy as np\n",
        "import mmcv, cv2\n",
        "from PIL import Image, ImageDraw\n",
        "from IPython import display"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0vX94h1EaQH"
      },
      "source": [
        "### Determine if an nvidia GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z0YFHFmEcwf",
        "outputId": "ae90d9ef-a876-46cf-e11d-2446d279cac9"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLVj-T5xFcXf"
      },
      "source": [
        "## The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZMoQsuFabi"
      },
      "source": [
        "### Define MTCNN module\n",
        "Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
        "\n",
        "See help(MTCNN) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8G9Tvg9Fd3J"
      },
      "source": [
        "mtcnn = MTCNN(keep_all=True, device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0asu0op8FpZn"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b8WVBwMRoXO"
      },
      "source": [
        "### Test 1 - video from git repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL04M9sfFkph"
      },
      "source": [
        "#### Get a sample video\n",
        "We begin by loading a video with some faces in it. The mmcv PyPI package by mmlabs is used to read the video frames (it can be installed with pip install mmcv). Frames are then converted to PIL images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-anZkxOgFqt5"
      },
      "source": [
        "video = mmcv.VideoReader('examples/video.mp4')\n",
        "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
        "\n",
        "# display.Video('examples/video.mp4', width=640)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_u8AzuRHGa2"
      },
      "source": [
        "#### Run video through MTCNN\n",
        "We iterate through each frame, detect faces, and draw their bounding boxes on the video frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uMYpt_bHFwF",
        "outputId": "5ef9f751-8416-4e89-e287-afd04370c17b"
      },
      "source": [
        "frames_tracked = []\n",
        "for i, frame in enumerate(frames):\n",
        "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
        "    \n",
        "    # Detect faces\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    \n",
        "    # Draw faces\n",
        "    frame_draw = frame.copy()\n",
        "    draw = ImageDraw.Draw(frame_draw)\n",
        "    for box in boxes:\n",
        "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
        "    \n",
        "    # Add to frame list\n",
        "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
        "print('\\nDone')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tracking frame: 105\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciRWIu8jHMyK"
      },
      "source": [
        "#### Display detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1oUrQtyHNjF"
      },
      "source": [
        "d = display.display(frames_tracked[0], display_id=True)\n",
        "i = 1\n",
        "try:\n",
        "    while True:\n",
        "        d.update(frames_tracked[i % len(frames_tracked)])\n",
        "        i += 1\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoVu67iHHUz6"
      },
      "source": [
        "#### Save tracked video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjd5KH55HVst"
      },
      "source": [
        "dim = frames_tracked[0].size\n",
        "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
        "video_tracked = cv2.VideoWriter('my_video_tracked.mp4', fourcc, 25.0, dim)\n",
        "for frame in frames_tracked:\n",
        "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
        "video_tracked.release()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehJtZZjZRSh3"
      },
      "source": [
        "### Test 2 - video from my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ08mXV6Oy4d",
        "outputId": "0010f86c-d9d4-40f8-f73c-e6817250e565"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctrJG23tPB_l"
      },
      "source": [
        "! cp /content/drive/MyDrive/faces_Trim.mp4 ./examples/"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bXJHyyrRbio"
      },
      "source": [
        "#### Get a sample video\n",
        "We begin by loading a video with some faces in it. The mmcv PyPI package by mmlabs is used to read the video frames (it can be installed with pip install mmcv). Frames are then converted to PIL images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVTj6uFjPQ4Q"
      },
      "source": [
        "video = mmcv.VideoReader('examples/faces_Trim.mp4')\n",
        "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdE7fJRxRegX"
      },
      "source": [
        "#### Run video through MTCNN\n",
        "We iterate through each frame, detect faces, and draw their bounding boxes on the video frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlFfnIWPT46",
        "outputId": "ec14b327-1918-4f01-bdb2-705f88ce7a41"
      },
      "source": [
        "frames_tracked = []\n",
        "for i, frame in enumerate(frames):\n",
        "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
        "    \n",
        "    # Detect faces\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    \n",
        "    # Draw faces\n",
        "    frame_draw = frame.copy()\n",
        "    draw = ImageDraw.Draw(frame_draw)\n",
        "    for box in boxes:\n",
        "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
        "    \n",
        "    # Add to frame list\n",
        "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
        "print('\\nDone')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tracking frame: 281\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3776DRiRhD4"
      },
      "source": [
        "#### Save tracked video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQadMrNoPWio"
      },
      "source": [
        "dim = frames_tracked[0].size\n",
        "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
        "video_tracked = cv2.VideoWriter('faces_Trim_video_tracked.mp4', fourcc, 25.0, dim)\n",
        "for frame in frames_tracked:\n",
        "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
        "video_tracked.release()"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}